{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc47e67",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a7f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Chạy pipeline cho dataset: desharnais\n",
      "🔍 Tìm siêu tham số tối ưu bằng PSO...\n",
      "\n",
      "🔁 Iteration 1/10\n",
      "✅ Cập nhật g_best: Score = 0.1415\n",
      "✅ Cập nhật g_best: Score = 0.1396\n",
      "✅ Cập nhật g_best: Score = 0.1393\n",
      "\n",
      "🔁 Iteration 2/10\n",
      "✅ Cập nhật g_best: Score = 0.1292\n",
      "✅ Cập nhật g_best: Score = 0.0890\n",
      "\n",
      "🔁 Iteration 3/10\n",
      "✅ Cập nhật g_best: Score = 0.0839\n",
      "\n",
      "🔁 Iteration 4/10\n",
      "✅ Cập nhật g_best: Score = 0.0797\n",
      "\n",
      "🔁 Iteration 5/10\n",
      "\n",
      "🔁 Iteration 6/10\n",
      "\n",
      "🔁 Iteration 7/10\n",
      "\n",
      "🔁 Iteration 8/10\n",
      "\n",
      "🔁 Iteration 9/10\n",
      "\n",
      "🔁 Iteration 10/10\n",
      "🏆 Siêu tham số tốt nhất: {'filters': 5, 'l2_reg': 0.001, 'dense_units': 24, 'dropout_rate': np.float64(0.17205939504736156), 'learning_rate': np.float64(0.006348859157055838), 'batch_size': 13, 'epochs': 107}\n",
      "📉 Score tốt nhất: 0.0797\n",
      "\n",
      "📂 Fold 1/3\n",
      "✅ Fold 1 RMSE: 0.1536\n",
      "\n",
      "📂 Fold 2/3\n",
      "✅ Fold 2 RMSE: 0.2019\n",
      "\n",
      "📂 Fold 3/3\n",
      "✅ Fold 3 RMSE: 0.0938\n",
      "\n",
      "📈 Kết quả đánh giá trên tập test:\n",
      "📌 MAE    : 0.1781\n",
      "📌 MSE    : 0.1376\n",
      "📌 RMSE   : 0.3710\n",
      "📌 R2     : 0.9029\n",
      "📌 MMRE   : 0.4788\n",
      "📌 MDMRE  : 0.0922\n",
      "📌 PRED25 : 64.7059\n",
      "\n",
      "Đã lưu kết quả vào 'results/desharnais_results.csv'\n",
      "Đã lưu mô hình vào 'models/desharnais_model.keras'\n",
      "Đã lưu hình ảnh trực quan hóa vào 'visualizations/desharnais_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Input, BatchNormalization, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "# Thiết lập seed để tái lập\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cấu hình bật/tắt các bước\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Bật/tắt tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "    'normalize_features': False,   # Bật/tắt chuẩn hóa đặc trưng\n",
    "    'log_transform_target': False, # Bật/tắt biến đổi log cho biến mục tiêu\n",
    "    'save_model': True,            # Bật/tắt lưu mô hình\n",
    "    'save_results': True,          # Bật/tắt lưu kết quả đánh giá\n",
    "    'visualize_results': True,     # Bật/tắt trực quan hóa\n",
    "    'noise_factor': 0.01,          # Độ lớn nhiễu Gaussian\n",
    "    'num_noise_copies': 2,         # Số bản sao nhiễu\n",
    "    'test_size': 0.15,             # Tỷ lệ tập test\n",
    "    'k_folds': 3,                  # Số fold trong cross-validation\n",
    "}\n",
    "\n",
    "# Hàm tính các chỉ số đánh giá\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# Hàm đọc và tiền xử lý dữ liệu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiểm tra cột tồn tại\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiểm tra kiểu dữ liệu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiểm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuẩn hóa đặc trưng nếu bật\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biến đổi log cho mục tiêu nếu bật\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# Hàm tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# Hàm xây dựng mô hình CNN\n",
    "def build_cnn_model(input_shape, filters=8, l2_reg=0.01, dense_units=16, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(filters, kernel_size=2, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters, kernel_size=2, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Không gian siêu tham số\n",
    "param_bounds = {\n",
    "    'filters': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dense_units': (8, 32),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# Hàm mã hóa & giải mã particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['filters'][0], param_bounds['filters'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'filters': int(particle[0]),\n",
    "        'l2_reg': particle[1],\n",
    "        'dense_units': int(particle[2]),\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# Hàm fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_cnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Hàm chạy PSO\n",
    "def run_pso_cnn(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\n🔁 Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][1] = max(particles[i][1], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"✅ Cập nhật g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Hàm huấn luyện và đánh giá mô hình tối ưu\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\n📂 Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"✅ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # Đánh giá trên tập test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # Tính trung bình lịch sử huấn luyện\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# Hàm trực quan hóa kết quả\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bình\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# Hàm chính để chạy pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\n🚀 Chạy pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Đọc và tiền xử lý dữ liệu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # Tăng cường dữ liệu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    X_aug = X_aug.reshape(X_aug.shape[0], X_aug.shape[1], 1)\n",
    "    \n",
    "    # Chia tập train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Chạy PSO\n",
    "    print(\"🔍 Tìm siêu tham số tối ưu bằng PSO...\")\n",
    "    best_particle, best_score = run_pso_cnn(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"🏆 Siêu tham số tốt nhất: {best_params}\")\n",
    "    print(f\"📉 Score tốt nhất: {best_score:.4f}\")\n",
    "    \n",
    "    # Huấn luyện và đánh giá\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\n📈 Kết quả đánh giá trên tập test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"📌 {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"📌 {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_results.csv', index=False)\n",
    "        print(f\"\\nĐã lưu kết quả vào 'results/{dataset_name}_results.csv'\")\n",
    "    \n",
    "    # Lưu mô hình\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_model.keras')\n",
    "        print(f\"Đã lưu mô hình vào 'models/{dataset_name}_model.keras'\")\n",
    "    \n",
    "    # Trực quan hóa\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"Đã lưu hình ảnh trực quan hóa vào 'visualizations/{dataset_name}_results.png'\")\n",
    "\n",
    "# Cấu hình các dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bab446",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17844f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Chạy pipeline cho dataset: desharnais\n",
      "🔍 Tìm siêu tham số tối ưu bằng PSO...\n",
      "\n",
      "🔁 Iteration 1/10\n",
      "✅ Cập nhật g_best: Score = 0.1859\n",
      "\n",
      "🔁 Iteration 2/10\n",
      "\n",
      "🔁 Iteration 3/10\n",
      "✅ Cập nhật g_best: Score = 0.1592\n",
      "\n",
      "🔁 Iteration 4/10\n",
      "\n",
      "🔁 Iteration 5/10\n",
      "\n",
      "🔁 Iteration 6/10\n",
      "\n",
      "🔁 Iteration 7/10\n",
      "\n",
      "🔁 Iteration 8/10\n",
      "\n",
      "🔁 Iteration 9/10\n",
      "\n",
      "🔁 Iteration 10/10\n",
      "🏆 Siêu tham số tốt nhất: {'hidden_units1': 35, 'hidden_units2': 11, 'l2_reg': np.float64(0.045730786735993637), 'dropout_rate': np.float64(0.243068412372354), 'learning_rate': np.float64(0.0044283623738321475), 'batch_size': 18, 'epochs': 71}\n",
      "📉 Score tốt nhất: 0.1592\n",
      "\n",
      "📂 Fold 1/3\n",
      "✅ Fold 1 RMSE: 0.3559\n",
      "\n",
      "📂 Fold 2/3\n",
      "✅ Fold 2 RMSE: 0.0884\n",
      "\n",
      "📂 Fold 3/3\n",
      "✅ Fold 3 RMSE: 0.0669\n",
      "\n",
      "📈 Kết quả đánh giá trên tập test:\n",
      "📌 MAE    : 0.0634\n",
      "📌 MSE    : 0.0058\n",
      "📌 RMSE   : 0.0760\n",
      "📌 R2     : 0.9959\n",
      "📌 MMRE   : 0.5409\n",
      "📌 MDMRE  : 0.0520\n",
      "📌 PRED25 : 88.2353\n",
      "\n",
      "Đã lưu kết quả vào 'results/desharnais_mlp_results.csv'\n",
      "Đã lưu mô hình vào 'models/desharnais_mlp_model.keras'\n",
      "Đã lưu hình ảnh trực quan hóa vào 'visualizations/desharnais_mlp_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thiết lập seed để tái lập\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cấu hình bật/tắt các bước\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Bật/tắt tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "    'normalize_features': False,   # Bật/tắt chuẩn hóa đặc trưng\n",
    "    'log_transform_target': False, # Bật/tắt biến đổi log cho biến mục tiêu\n",
    "    'save_model': True,            # Bật/tắt lưu mô hình\n",
    "    'save_results': True,          # Bật/tắt lưu kết quả đánh giá\n",
    "    'visualize_results': True,     # Bật/tắt trực quan hóa\n",
    "    'noise_factor': 0.01,          # Độ lớn nhiễu Gaussian\n",
    "    'num_noise_copies': 2,         # Số bản sao nhiễu\n",
    "    'test_size': 0.15,             # Tỷ lệ tập test\n",
    "    'k_folds': 3,                  # Số fold trong cross-validation\n",
    "}\n",
    "\n",
    "# Hàm tính các chỉ số đánh giá\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# Hàm đọc và tiền xử lý dữ liệu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiểm tra cột tồn tại\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiểm tra kiểu dữ liệu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiểm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuẩn hóa đặc trưng nếu bật\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biến đổi log cho mục tiêu nếu bật\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# Hàm tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# Hàm xây dựng mô hình MLP\n",
    "def build_mlp_model(input_dim, hidden_units1=16, hidden_units2=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(hidden_units1, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(hidden_units2, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Không gian siêu tham số cho MLP\n",
    "param_bounds = {\n",
    "    'hidden_units1': (8, 32),\n",
    "    'hidden_units2': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# Hàm mã hóa & giải mã particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['hidden_units1'][0], param_bounds['hidden_units1'][1] + 1),\n",
    "        np.random.randint(param_bounds['hidden_units2'][0], param_bounds['hidden_units2'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'hidden_units1': int(particle[0]),\n",
    "        'hidden_units2': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# Hàm fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_mlp_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Hàm chạy PSO\n",
    "def run_pso_mlp(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\n🔁 Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"✅ Cập nhật g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Hàm huấn luyện và đánh giá mô hình tối ưu\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_mlp_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\n📂 Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"✅ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # Đánh giá trên tập test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # Tính trung bình lịch sử huấn luyện\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# Hàm trực quan hóa kết quả\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bình\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_mlp_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# Hàm chính để chạy pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\n🚀 Chạy pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Đọc và tiền xử lý dữ liệu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # Tăng cường dữ liệu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Chia tập train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Chạy PSO\n",
    "    print(\"🔍 Tìm siêu tham số tối ưu bằng PSO...\")\n",
    "    best_particle, best_score = run_pso_mlp(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"🏆 Siêu tham số tốt nhất: {best_params}\")\n",
    "    print(f\"📉 Score tốt nhất: {best_score:.4f}\")\n",
    "    \n",
    "    # Huấn luyện và đánh giá\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\n📈 Kết quả đánh giá trên tập test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"📌 {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"📌 {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_mlp_results.csv', index=False)\n",
    "        print(f\"\\nĐã lưu kết quả vào 'results/{dataset_name}_mlp_results.csv'\")\n",
    "    \n",
    "    # Lưu mô hình\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_mlp_model.keras')\n",
    "        print(f\"Đã lưu mô hình vào 'models/{dataset_name}_mlp_model.keras'\")\n",
    "    \n",
    "    # Trực quan hóa\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"Đã lưu hình ảnh trực quan hóa vào 'visualizations/{dataset_name}_mlp_results.png'\")\n",
    "\n",
    "# Cấu hình các dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Chạy pipeline cho từng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851a234",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Chạy pipeline cho dataset: desharnais\n",
      "🔍 Tìm siêu tham số tối ưu bằng PSO...\n",
      "\n",
      "🔁 Iteration 1/10\n",
      "✅ Cập nhật g_best: Score = 0.2331\n",
      "✅ Cập nhật g_best: Score = 0.2070\n",
      "\n",
      "🔁 Iteration 2/10\n",
      "✅ Cập nhật g_best: Score = 0.2040\n",
      "✅ Cập nhật g_best: Score = 0.1954\n",
      "✅ Cập nhật g_best: Score = 0.1946\n",
      "\n",
      "🔁 Iteration 3/10\n",
      "✅ Cập nhật g_best: Score = 0.1709\n",
      "\n",
      "🔁 Iteration 4/10\n",
      "\n",
      "🔁 Iteration 5/10\n",
      "\n",
      "🔁 Iteration 6/10\n",
      "\n",
      "🔁 Iteration 7/10\n",
      "\n",
      "🔁 Iteration 8/10\n",
      "\n",
      "🔁 Iteration 9/10\n",
      "\n",
      "🔁 Iteration 10/10\n",
      "🏆 Siêu tham số tốt nhất: {'lstm_units': 17, 'dense_units': 10, 'l2_reg': np.float64(0.05290511907499294), 'dropout_rate': np.float64(0.2388252925394604), 'learning_rate': np.float64(0.01028603584658312), 'batch_size': 13, 'epochs': 105}\n",
      "📉 Score tốt nhất: 0.1709\n",
      "\n",
      "📂 Fold 1/3\n",
      "✅ Fold 1 RMSE: 0.1741\n",
      "\n",
      "📂 Fold 2/3\n",
      "✅ Fold 2 RMSE: 0.1602\n",
      "\n",
      "📂 Fold 3/3\n",
      "✅ Fold 3 RMSE: 0.1102\n",
      "\n",
      "📈 Kết quả đánh giá trên tập test:\n",
      "📌 MAE    : 0.1038\n",
      "📌 MSE    : 0.0185\n",
      "📌 RMSE   : 0.1361\n",
      "📌 R2     : 0.9869\n",
      "📌 MMRE   : 3.4108\n",
      "📌 MDMRE  : 0.0612\n",
      "📌 PRED25 : 88.2353\n",
      "\n",
      "Đã lưu kết quả vào 'results/desharnais_lstm_results.csv'\n",
      "Đã lưu mô hình vào 'models/desharnais_lstm_model.keras'\n",
      "Đã lưu hình ảnh trực quan hóa vào 'visualizations/desharnais_lstm_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thiết lập seed để tái lập\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cấu hình bật/tắt các bước\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Bật/tắt tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "    'normalize_features': False,   # Bật/tắt chuẩn hóa đặc trưng\n",
    "    'log_transform_target': False, # Bật/tắt biến đổi log cho biến mục tiêu\n",
    "    'save_model': True,            # Bật/tắt lưu mô hình\n",
    "    'save_results': True,          # Bật/tắt lưu kết quả đánh giá\n",
    "    'visualize_results': True,     # Bật/tắt trực quan hóa\n",
    "    'noise_factor': 0.01,          # Độ lớn nhiễu Gaussian\n",
    "    'num_noise_copies': 2,         # Số bản sao nhiễu\n",
    "    'test_size': 0.15,             # Tỷ lệ tập test\n",
    "    'k_folds': 3,                  # Số fold trong cross-validation\n",
    "    'timesteps': 1                 # Số bước thời gian cho LSTM\n",
    "}\n",
    "\n",
    "# Hàm tính các chỉ số đánh giá\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# Hàm đọc và tiền xử lý dữ liệu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiểm tra cột tồn tại\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiểm tra kiểu dữ liệu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiểm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuẩn hóa đặc trưng nếu bật\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biến đổi log cho mục tiêu nếu bật\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# Hàm tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# Hàm reshape dữ liệu cho LSTM\n",
    "def reshape_for_lstm(X, timesteps=1):\n",
    "    # Reshape dữ liệu thành [samples, timesteps, features]\n",
    "    n_samples, n_features = X.shape\n",
    "    n_timesteps = timesteps\n",
    "    n_new_samples = n_samples // n_timesteps\n",
    "    X_reshaped = X[:n_new_samples * n_timesteps].reshape(n_new_samples, n_timesteps, n_features)\n",
    "    return X_reshaped\n",
    "\n",
    "# Hàm xây dựng mô hình LSTM\n",
    "def build_lstm_model(input_shape, lstm_units=16, dense_units=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(lstm_units, activation='tanh', recurrent_activation='sigmoid', return_sequences=False,\n",
    "             kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Không gian siêu tham số cho LSTM\n",
    "param_bounds = {\n",
    "    'lstm_units': (8, 32),\n",
    "    'dense_units': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# Hàm mã hóa & giải mã particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['lstm_units'][0], param_bounds['lstm_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'lstm_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# Hàm fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_lstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Hàm chạy PSO\n",
    "def run_pso_lstm(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\n🔁 Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"✅ Cập nhật g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Hàm huấn luyện và đánh giá mô hình tối ưu\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_lstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\n📂 Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"✅ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # Đánh giá trên tập test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # Tính trung bình lịch sử huấn luyện\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# Hàm trực quan hóa kết quả\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bình\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_lstm_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# Hàm chính để chạy pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\n🚀 Chạy pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Đọc và tiền xử lý dữ liệu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # Tăng cường dữ liệu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Reshape dữ liệu cho LSTM\n",
    "    X_aug = reshape_for_lstm(X_aug, CONFIG['timesteps'])\n",
    "    y_aug = y_aug[:X_aug.shape[0]]\n",
    "    \n",
    "    # Chia tập train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Chạy PSO\n",
    "    print(\"🔍 Tìm siêu tham số tối ưu bằng PSO...\")\n",
    "    best_particle, best_score = run_pso_lstm(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"🏆 Siêu tham số tốt nhất: {best_params}\")\n",
    "    print(f\"📉 Score tốt nhất: {best_score:.4f}\")\n",
    "    \n",
    "    # Huấn luyện và đánh giá\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\n📈 Kết quả đánh giá trên tập test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"📌 {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"📌 {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_lstm_results.csv', index=False)\n",
    "        print(f\"\\nĐã lưu kết quả vào 'results/{dataset_name}_lstm_results.csv'\")\n",
    "    \n",
    "    # Lưu mô hình\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_lstm_model.keras')\n",
    "        print(f\"Đã lưu mô hình vào 'models/{dataset_name}_lstm_model.keras'\")\n",
    "    \n",
    "    # Trực quan hóa\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"Đã lưu hình ảnh trực quan hóa vào 'visualizations/{dataset_name}_lstm_results.png'\")\n",
    "\n",
    "# Cấu hình các dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Chạy pipeline cho từng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a647c",
   "metadata": {},
   "source": [
    "# RBFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Chạy pipeline cho dataset: desharnais\n",
      "🔍 Tìm siêu tham số tối ưu bằng PSO...\n",
      "\n",
      "🔁 Iteration 1/10\n",
      "✅ Cập nhật g_best: Score = 0.4931\n",
      "✅ Cập nhật g_best: Score = 0.4696\n",
      "✅ Cập nhật g_best: Score = 0.4233\n",
      "\n",
      "🔁 Iteration 2/10\n",
      "✅ Cập nhật g_best: Score = 0.3826\n",
      "✅ Cập nhật g_best: Score = 0.3691\n",
      "✅ Cập nhật g_best: Score = 0.3667\n",
      "\n",
      "🔁 Iteration 3/10\n",
      "✅ Cập nhật g_best: Score = 0.3663\n",
      "✅ Cập nhật g_best: Score = 0.3646\n",
      "✅ Cập nhật g_best: Score = 0.3622\n",
      "\n",
      "🔁 Iteration 4/10\n",
      "✅ Cập nhật g_best: Score = 0.3620\n",
      "\n",
      "🔁 Iteration 5/10\n",
      "\n",
      "🔁 Iteration 6/10\n",
      "\n",
      "🔁 Iteration 7/10\n",
      "✅ Cập nhật g_best: Score = 0.3596\n",
      "\n",
      "🔁 Iteration 8/10\n",
      "\n",
      "🔁 Iteration 9/10\n",
      "\n",
      "🔁 Iteration 10/10\n",
      "🏆 Siêu tham số tốt nhất: {'num_centers': 22, 'sigma': 2.14376277835498, 'learning_rate': 0.008146268376391983, 'batch_size': 11, 'epochs': 43}\n",
      "📉 Score tốt nhất: 0.3596\n",
      "\n",
      "📂 Fold 1/3\n",
      "✅ Fold 1 RMSE: 0.3454\n",
      "\n",
      "📂 Fold 2/3\n",
      "✅ Fold 2 RMSE: 0.3723\n",
      "\n",
      "📂 Fold 3/3\n",
      "✅ Fold 3 RMSE: 0.3257\n",
      "\n",
      "📈 Kết quả đánh giá trên tập test:\n",
      "📌 MAE    : 0.3087\n",
      "📌 MSE    : 0.2034\n",
      "📌 RMSE   : 0.4510\n",
      "📌 R2     : 0.8565\n",
      "📌 MMRE   : 2.0215\n",
      "📌 MDMRE  : 0.3395\n",
      "📌 PRED25 : 41.1765\n",
      "\n",
      "Đã lưu kết quả vào 'results/desharnais_rbfn_results.csv'\n",
      "Đã lưu mô hình vào 'models/desharnais_rbfn_model.keras'\n",
      "Đã lưu hình ảnh trực quan hóa vào 'visualizations/desharnais_rbfn_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thiết lập seed để tái lập\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cấu hình bật/tắt các bước\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Bật/tắt tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "    'normalize_features': False,   # Bật/tắt chuẩn hóa đặc trưng\n",
    "    'log_transform_target': False, # Bật/tắt biến đổi log cho biến mục tiêu\n",
    "    'save_model': True,            # Bật/tắt lưu mô hình\n",
    "    'save_results': True,          # Bật/tắt lưu kết quả đánh giá\n",
    "    'visualize_results': True,     # Bật/tắt trực quan hóa\n",
    "    'noise_factor': 0.01,          # Độ lớn nhiễu Gaussian\n",
    "    'num_noise_copies': 2,         # Số bản sao nhiễu\n",
    "    'test_size': 0.15,             # Tỷ lệ tập test\n",
    "    'k_folds': 3,                  # Số fold trong cross-validation\n",
    "}\n",
    "\n",
    "# Hàm tính các chỉ số đánh giá\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# Hàm đọc và tiền xử lý dữ liệu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiểm tra cột tồn tại\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiểm tra kiểu dữ liệu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiểm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuẩn hóa đặc trưng nếu bật\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biến đổi log cho mục tiêu nếu bật\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# Hàm tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# Tầng RBF tùy chỉnh\n",
    "class RBFLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, sigma, centers, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.sigma = float(sigma)\n",
    "        self.centers = tf.Variable(centers, trainable=False, dtype=tf.float32, name='centers')\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.rbf_weights = self.add_weight(\n",
    "            name='rbf_weights',\n",
    "            shape=(self.units, 1),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.rbf_biases = self.add_weight(\n",
    "            name='rbf_biases',\n",
    "            shape=(1,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        diff = tf.expand_dims(inputs, axis=1) - tf.expand_dims(self.centers, axis=0)\n",
    "        l2 = tf.reduce_sum(tf.square(diff), axis=-1)\n",
    "        output = tf.exp(-l2 / (2.0 * self.sigma ** 2))\n",
    "        return tf.matmul(output, self.rbf_weights) + self.rbf_biases\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(RBFLayer, self).get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'sigma': float(self.sigma),\n",
    "            'centers': self.centers.numpy().tolist()\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Hàm xây dựng mô hình RBFN\n",
    "def build_rbfn_model(input_dim, num_centers=10, sigma=1.0, learning_rate=0.001, X_train=None):\n",
    "    if X_train is None:\n",
    "        raise ValueError(\"X_train must be provided to initialize centers with KMeans\")\n",
    "    \n",
    "    # Khởi tạo centers bằng KMeans\n",
    "    n_clusters = max(int(num_centers), 1)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(X_train)\n",
    "    centers = np.array(kmeans.cluster_centers_, dtype=np.float32)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        RBFLayer(units=int(num_centers), sigma=sigma, centers=centers, name='rbf_layer'),\n",
    "        Dense(1, activation='linear', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Không gian siêu tham số cho RBFN\n",
    "param_bounds = {\n",
    "    'num_centers': (5, 20),\n",
    "    'sigma': (0.1, 2.0),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# Hàm mã hóa & giải mã particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['num_centers'][0], param_bounds['num_centers'][1] + 1),\n",
    "        np.random.uniform(param_bounds['sigma'][0], param_bounds['sigma'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'num_centers': int(particle[0]),\n",
    "        'sigma': float(particle[1]),\n",
    "        'learning_rate': float(particle[2]),\n",
    "        'batch_size': int(particle[3]),\n",
    "        'epochs': int(particle[4])\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# Hàm fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_rbfn_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        X_train=X_train,\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Hàm chạy PSO\n",
    "def run_pso_rbfn(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\n🔁 Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][1] = max(particles[i][1], param_bounds['sigma'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"✅ Cập nhật g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Hàm huấn luyện và đánh giá mô hình tối ưu\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_rbfn_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        X_train=X_train,\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\n📂 Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"✅ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # Đánh giá trên tập test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # Tính trung bình lịch sử huấn luyện\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# Hàm trực quan hóa kết quả\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bình\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_rbfn_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# Hàm chính để chạy pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\n🚀 Chạy pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Đọc và tiền xử lý dữ liệu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # Tăng cường dữ liệu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Chia tập train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Chạy PSO\n",
    "    print(\"🔍 Tìm siêu tham số tối ưu bằng PSO...\")\n",
    "    best_particle, best_score = run_pso_rbfn(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"🏆 Siêu tham số tốt nhất: {best_params}\")\n",
    "    print(f\"📉 Score tốt nhất: {best_score:.4f}\")\n",
    "    \n",
    "    # Huấn luyện và đánh giá\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\n📈 Kết quả đánh giá trên tập test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"📌 {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"📌 {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_rbfn_results.csv', index=False)\n",
    "        print(f\"\\nĐã lưu kết quả vào 'results/{dataset_name}_rbfn_results.csv'\")\n",
    "    \n",
    "    # Lưu mô hình\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_rbfn_model.keras')\n",
    "        print(f\"Đã lưu mô hình vào 'models/{dataset_name}_rbfn_model.keras'\")\n",
    "    \n",
    "    # Trực quan hóa\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"Đã lưu hình ảnh trực quan hóa vào 'visualizations/{dataset_name}_rbfn_results.png'\")\n",
    "\n",
    "# Cấu hình các dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Chạy pipeline cho từng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18597b1b",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbde1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Chạy pipeline cho dataset: desharnais\n",
      "🔍 Tìm siêu tham số tối ưu bằng PSO...\n",
      "\n",
      "🔁 Iteration 1/10\n",
      "\n",
      "🔁 Iteration 2/10\n",
      "✅ Cập nhật g_best: Score = 0.1618\n",
      "\n",
      "🔁 Iteration 3/10\n",
      "✅ Cập nhật g_best: Score = 0.1569\n",
      "✅ Cập nhật g_best: Score = 0.1564\n",
      "\n",
      "🔁 Iteration 4/10\n",
      "✅ Cập nhật g_best: Score = 0.1502\n",
      "✅ Cập nhật g_best: Score = 0.1476\n",
      "\n",
      "🔁 Iteration 5/10\n",
      "✅ Cập nhật g_best: Score = 0.1470\n",
      "\n",
      "🔁 Iteration 6/10\n",
      "\n",
      "🔁 Iteration 7/10\n",
      "\n",
      "🔁 Iteration 8/10\n",
      "\n",
      "🔁 Iteration 9/10\n",
      "\n",
      "🔁 Iteration 10/10\n",
      "🏆 Siêu tham số tốt nhất: {'rnn_units': 19, 'dense_units': 6, 'l2_reg': np.float64(0.04578883850996157), 'dropout_rate': np.float64(0.1755520424929638), 'learning_rate': np.float64(0.009449979979976523), 'batch_size': 13, 'epochs': 104}\n",
      "📉 Score tốt nhất: 0.1470\n",
      "\n",
      "📌 Fold 1/3\n",
      "✅ Fold 1 RMSE: 0.1408\n",
      "\n",
      "📌 Fold 2/3\n",
      "✅ Fold 2 RMSE: 0.1700\n",
      "\n",
      "📌 Fold 3/3\n",
      "✅ Fold 3 RMSE: 0.1077\n",
      "\n",
      "📈 Kết quả đánh giá trên tập test:\n",
      "📌 MAE    : 0.0879\n",
      "📌 MSE    : 0.0145\n",
      "📌 RMSE   : 0.1205\n",
      "📌 R2     : 0.9897\n",
      "📌 MMRE   : 3.2531\n",
      "📌 MDMRE  : 0.0423\n",
      "📌 PRED25 : 88.2353\n",
      "\n",
      "Đã lưu kết quả vào 'results/desharnais_rnn_results.csv'\n",
      "Đã lưu mô hình vào 'models/desharnais_rnn_model.keras'\n",
      "Đã lưu hình ảnh trực quan hóa vào 'visualizations/desharnais_rnn_results.png'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thiết lập seed để tái lập\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cấu hình bật/tắt các bước\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Bật/tắt tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "    'normalize_features': False,   # Bật/tắt chuẩn hóa đặc trưng\n",
    "    'log_transform_target': False, # Bật/tắt biến đổi log cho biến mục tiêu\n",
    "    'save_model': True,            # Bật/tắt lưu mô hình\n",
    "    'save_results': True,          # Bật/tắt lưu kết quả đánh giá\n",
    "    'visualize_results': True,     # Bật/tắt trực quan hóa\n",
    "    'noise_factor': 0.01,          # Độ lớn nhiễu Gaussian\n",
    "    'num_noise_copies': 2,         # Số bản sao nhiễu\n",
    "    'test_size': 0.15,             # Tỷ lệ tập test\n",
    "    'k_folds': 3,                  # Số fold trong cross-validation\n",
    "    'timesteps': 1                 # Số bước thời gian cho RNN \n",
    "}\n",
    "\n",
    "# Hàm tính các chỉ số đánh giá\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# Hàm đọc và tiền xử lý dữ liệu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiểm tra cột tồn tại\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiểm tra kiểu dữ liệu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiểm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuẩn hóa đặc trưng nếu bật\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biến đổi log cho mục tiêu nếu bật\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# Hàm tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# Hàm reshape dữ liệu cho RNN\n",
    "def reshape_for_rnn(X, timesteps=1):\n",
    "    # Reshape dữ liệu thành [samples, timesteps, features]\n",
    "    n_samples, n_features = X.shape\n",
    "    n_timesteps = timesteps\n",
    "    n_new_samples = n_samples // n_timesteps\n",
    "    X_reshaped = X[:n_new_samples * n_timesteps].reshape(n_new_samples, n_timesteps, n_features)\n",
    "    return X_reshaped\n",
    "\n",
    "# Hàm xây dựng mô hình RNN\n",
    "def build_rnn_model(input_shape, rnn_units=16, dense_units=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        SimpleRNN(rnn_units, activation='tanh', return_sequences=False, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Không gian siêu tham số cho RNN\n",
    "param_bounds = {\n",
    "    'rnn_units': (8, 32),\n",
    "    'dense_units': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# Hàm mã hóa & giải mã particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['rnn_units'][0], param_bounds['rnn_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'rnn_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# Hàm fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_rnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Hàm chạy PSO\n",
    "def run_pso_rnn(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\n🔁 Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand() * np.ones(dim)\n",
    "            r2 = np.random.uniform(0, 1, dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, -1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"✅ Cập nhật g_best: Score = {g_best_score:.4f}\")\n",
    "                \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Hàm huấn luyện và đánh giá mô hình tối ưu\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_rnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\n📌 Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"✅ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # Đánh giá trên tập test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # Tính trung bình lịch sử huấn luyện\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# Hàm trực quan hóa kết quả\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bình\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_rnn_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# Hàm chính để chạy pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\n🚀 Chạy pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Đọc và tiền xử lý dữ liệu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # Tăng cường dữ liệu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Reshape dữ liệu cho RNN\n",
    "    X_aug = reshape_for_rnn(X_aug, CONFIG['timesteps'])\n",
    "    y_aug = y_aug[:X_aug.shape[0]]  # Cắt y để khớp số mẫu sau reshape\n",
    "    \n",
    "    # Chia tập train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Chạy PSO\n",
    "    print(\"🔍 Tìm siêu tham số tối ưu bằng PSO...\")\n",
    "    best_particle, best_score = run_pso_rnn(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"🏆 Siêu tham số tốt nhất: {best_params}\")\n",
    "    print(f\"📉 Score tốt nhất: {best_score:.4f}\")\n",
    "    \n",
    "    # Huấn luyện và đánh giá\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\n📈 Kết quả đánh giá trên tập test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"📌 {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"📌 {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_rnn_results.csv', index=False)\n",
    "        print(f\"\\nĐã lưu kết quả vào 'results/{dataset_name}_rnn_results.csv'\")\n",
    "    \n",
    "    # Lưu mô hình\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_rnn_model.keras')\n",
    "        print(f\"Đã lưu mô hình vào 'models/{dataset_name}_rnn_model.keras'\")\n",
    "    \n",
    "    # Trực quan hóa\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"Đã lưu hình ảnh trực quan hóa vào 'visualizations/{dataset_name}_rnn_results.png'\")\n",
    "\n",
    "# Cấu hình các dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Chạy pipeline cho từng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b6f04",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3850322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Chạy pipeline cho dataset: desharnais\n",
      "🔍 Tìm siêu tham số tối ưu bằng PSO...\n",
      "\n",
      "🔁 Iteration 1/10\n",
      "✅ Cập nhật g_best: Score = 0.1341\n",
      "\n",
      "🔁 Iteration 2/10\n",
      "\n",
      "🔁 Iteration 3/10\n",
      "✅ Cập nhật g_best: Score = 0.1324\n",
      "\n",
      "🔁 Iteration 4/10\n",
      "\n",
      "🔁 Iteration 5/10\n",
      "\n",
      "🔁 Iteration 6/10\n",
      "\n",
      "🔁 Iteration 7/10\n",
      "\n",
      "🔁 Iteration 8/10\n",
      "\n",
      "🔁 Iteration 9/10\n",
      "\n",
      "🔁 Iteration 10/10\n",
      "🏆 Siêu tham số tốt nhất: {'lstm_units': 10, 'dense_units': 1, 'l2_reg': np.float64(0.018236164417873006), 'dropout_rate': np.float64(0.32471572716955666), 'learning_rate': np.float64(0.005911359310043052), 'batch_size': 13, 'epochs': 112}\n",
      "📉 Score tốt nhất: 0.1324\n",
      "\n",
      "📂 Fold 1/3\n",
      "✅ Fold 1 RMSE: 0.3708\n",
      "\n",
      "📂 Fold 2/3\n",
      "✅ Fold 2 RMSE: 0.3822\n",
      "\n",
      "📂 Fold 3/3\n",
      "✅ Fold 3 RMSE: 0.2721\n",
      "\n",
      "📈 Kết quả đánh giá trên tập test:\n",
      "📌 MAE    : 0.3196\n",
      "📌 MSE    : 0.1606\n",
      "📌 RMSE   : 0.4008\n",
      "📌 R2     : 0.8867\n",
      "📌 MMRE   : 2.4827\n",
      "📌 MDMRE  : 0.3384\n",
      "📌 PRED25 : 29.4118\n",
      "\n",
      "Đã lưu kết quả vào 'results/desharnais_bilstm_results.csv'\n",
      "Đã lưu mô hình vào 'models/desharnais_bilstm_model.keras'\n",
      "Đã lưu hình ảnh trực quan hóa vào 'visualizations/desharnais_bilstm_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thiết lập seed để tái lập\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cấu hình bật/tắt các bước\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Bật/tắt tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "    'normalize_features': False,   # Bật/tắt chuẩn hóa đặc trưng\n",
    "    'log_transform_target': False, # Bật/tắt biến đổi log cho biến mục tiêu\n",
    "    'save_model': True,            # Bật/tắt lưu mô hình\n",
    "    'save_results': True,          # Bật/tắt lưu kết quả đánh giá\n",
    "    'visualize_results': True,     # Bật/tắt trực quan hóa\n",
    "    'noise_factor': 0.01,          # Độ lớn nhiễu Gaussian\n",
    "    'num_noise_copies': 2,         # Số bản sao nhiễu\n",
    "    'test_size': 0.15,             # Tỷ lệ tập test\n",
    "    'k_folds': 3,                  # Số fold trong cross-validation\n",
    "    'timesteps': 1                 # Số bước thời gian cho BiLSTM \n",
    "}\n",
    "\n",
    "# Hàm tính các chỉ số đánh giá\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# Hàm đọc và tiền xử lý dữ liệu\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiểm tra cột tồn tại\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiểm tra kiểu dữ liệu\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiểm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuẩn hóa đặc trưng nếu bật\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biến đổi log cho mục tiêu nếu bật\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# Hàm tăng cường dữ liệu bằng nhiễu Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# Hàm reshape dữ liệu cho BiLSTM\n",
    "def reshape_for_bilstm(X, timesteps=1):\n",
    "    # Reshape dữ liệu thành [samples, timesteps, features]\n",
    "    n_samples, n_features = X.shape\n",
    "    n_timesteps = timesteps\n",
    "    n_new_samples = n_samples // n_timesteps\n",
    "    X_reshaped = X[:n_new_samples * n_timesteps].reshape(n_new_samples, n_timesteps, n_features)\n",
    "    return X_reshaped\n",
    "\n",
    "# Hàm xây dựng mô hình BiLSTM\n",
    "def build_bilstm_model(input_shape, lstm_units=16, dense_units=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=False, kernel_regularizer=l2(l2_reg))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Không gian siêu tham số cho BiLSTM\n",
    "param_bounds = {\n",
    "    'lstm_units': (8, 16),  \n",
    "    'dense_units': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (4, 16), \n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# Hàm mã hóa & giải mã particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['lstm_units'][0], param_bounds['lstm_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'lstm_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# Hàm fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_bilstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# Hàm chạy PSO\n",
    "def run_pso_bilstm(X_train, y_train, num_particles=10, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\n🔁 Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"✅ Cập nhật g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# Hàm huấn luyện và đánh giá mô hình tối ưu\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_bilstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\n📂 Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"✅ Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # Đánh giá trên tập test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # Tính trung bình lịch sử huấn luyện\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# Hàm trực quan hóa kết quả\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bình\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_bilstm_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# Hàm chính để chạy pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\n🚀 Chạy pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Đọc và tiền xử lý dữ liệu\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # Tăng cường dữ liệu\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Reshape dữ liệu cho BiLSTM\n",
    "    X_aug = reshape_for_bilstm(X_aug, CONFIG['timesteps'])\n",
    "    y_aug = y_aug[:X_aug.shape[0]]  # Cắt y để khớp số mẫu sau reshape\n",
    "    \n",
    "    # Chia tập train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Chạy PSO\n",
    "    print(\"🔍 Tìm siêu tham số tối ưu bằng PSO...\")\n",
    "    best_particle, best_score = run_pso_bilstm(X_train, y_train, num_particles=10, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"🏆 Siêu tham số tốt nhất: {best_params}\")\n",
    "    print(f\"📉 Score tốt nhất: {best_score:.4f}\")\n",
    "    \n",
    "    # Huấn luyện và đánh giá\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In kết quả\n",
    "    print(\"\\n📈 Kết quả đánh giá trên tập test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"📌 {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"📌 {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # Lưu kết quả\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_bilstm_results.csv', index=False)\n",
    "        print(f\"\\nĐã lưu kết quả vào 'results/{dataset_name}_bilstm_results.csv'\")\n",
    "    \n",
    "    # Lưu mô hình\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_bilstm_model.keras')\n",
    "        print(f\"Đã lưu mô hình vào 'models/{dataset_name}_bilstm_model.keras'\")\n",
    "    \n",
    "    # Trực quan hóa\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"Đã lưu hình ảnh trực quan hóa vào 'visualizations/{dataset_name}_bilstm_results.png'\")\n",
    "\n",
    "# Cấu hình các dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Chạy pipeline cho từng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

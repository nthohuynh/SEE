{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc47e67",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a7f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Cháº¡y pipeline cho dataset: desharnais\n",
      "ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\n",
      "\n",
      "ğŸ” Iteration 1/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1415\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1396\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1393\n",
      "\n",
      "ğŸ” Iteration 2/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1292\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.0890\n",
      "\n",
      "ğŸ” Iteration 3/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.0839\n",
      "\n",
      "ğŸ” Iteration 4/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.0797\n",
      "\n",
      "ğŸ” Iteration 5/10\n",
      "\n",
      "ğŸ” Iteration 6/10\n",
      "\n",
      "ğŸ” Iteration 7/10\n",
      "\n",
      "ğŸ” Iteration 8/10\n",
      "\n",
      "ğŸ” Iteration 9/10\n",
      "\n",
      "ğŸ” Iteration 10/10\n",
      "ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {'filters': 5, 'l2_reg': 0.001, 'dense_units': 24, 'dropout_rate': np.float64(0.17205939504736156), 'learning_rate': np.float64(0.006348859157055838), 'batch_size': 13, 'epochs': 107}\n",
      "ğŸ“‰ Score tá»‘t nháº¥t: 0.0797\n",
      "\n",
      "ğŸ“‚ Fold 1/3\n",
      "âœ… Fold 1 RMSE: 0.1536\n",
      "\n",
      "ğŸ“‚ Fold 2/3\n",
      "âœ… Fold 2 RMSE: 0.2019\n",
      "\n",
      "ğŸ“‚ Fold 3/3\n",
      "âœ… Fold 3 RMSE: 0.0938\n",
      "\n",
      "ğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\n",
      "ğŸ“Œ MAE    : 0.1781\n",
      "ğŸ“Œ MSE    : 0.1376\n",
      "ğŸ“Œ RMSE   : 0.3710\n",
      "ğŸ“Œ R2     : 0.9029\n",
      "ğŸ“Œ MMRE   : 0.4788\n",
      "ğŸ“Œ MDMRE  : 0.0922\n",
      "ğŸ“Œ PRED25 : 64.7059\n",
      "\n",
      "ÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/desharnais_results.csv'\n",
      "ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/desharnais_model.keras'\n",
      "ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/desharnais_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Input, BatchNormalization, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "# Thiáº¿t láº­p seed Ä‘á»ƒ tÃ¡i láº­p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cáº¥u hÃ¬nh báº­t/táº¯t cÃ¡c bÆ°á»›c\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Báº­t/táº¯t tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "    'normalize_features': False,   # Báº­t/táº¯t chuáº©n hÃ³a Ä‘áº·c trÆ°ng\n",
    "    'log_transform_target': False, # Báº­t/táº¯t biáº¿n Ä‘á»•i log cho biáº¿n má»¥c tiÃªu\n",
    "    'save_model': True,            # Báº­t/táº¯t lÆ°u mÃ´ hÃ¬nh\n",
    "    'save_results': True,          # Báº­t/táº¯t lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "    'visualize_results': True,     # Báº­t/táº¯t trá»±c quan hÃ³a\n",
    "    'noise_factor': 0.01,          # Äá»™ lá»›n nhiá»…u Gaussian\n",
    "    'num_noise_copies': 2,         # Sá»‘ báº£n sao nhiá»…u\n",
    "    'test_size': 0.15,             # Tá»· lá»‡ táº­p test\n",
    "    'k_folds': 3,                  # Sá»‘ fold trong cross-validation\n",
    "}\n",
    "\n",
    "# HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# HÃ m Ä‘á»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiá»ƒm tra cá»™t tá»“n táº¡i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiá»ƒm tra kiá»ƒu dá»¯ liá»‡u\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiá»ƒm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuáº©n hÃ³a Ä‘áº·c trÆ°ng náº¿u báº­t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biáº¿n Ä‘á»•i log cho má»¥c tiÃªu náº¿u báº­t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# HÃ m tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# HÃ m xÃ¢y dá»±ng mÃ´ hÃ¬nh CNN\n",
    "def build_cnn_model(input_shape, filters=8, l2_reg=0.01, dense_units=16, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(filters, kernel_size=2, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(filters, kernel_size=2, activation='relu', padding='same', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# KhÃ´ng gian siÃªu tham sá»‘\n",
    "param_bounds = {\n",
    "    'filters': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dense_units': (8, 32),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a & giáº£i mÃ£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['filters'][0], param_bounds['filters'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'filters': int(particle[0]),\n",
    "        'l2_reg': particle[1],\n",
    "        'dense_units': int(particle[2]),\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# HÃ m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_cnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# HÃ m cháº¡y PSO\n",
    "def run_pso_cnn(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nğŸ” Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][1] = max(particles[i][1], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"âœ… Cáº­p nháº­t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# HÃ m huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh tá»‘i Æ°u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nğŸ“‚ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"âœ… Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # TÃ­nh trung bÃ¬nh lá»‹ch sá»­ huáº¥n luyá»‡n\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# HÃ m trá»±c quan hÃ³a káº¿t quáº£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bÃ¬nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# HÃ m chÃ­nh Ä‘á»ƒ cháº¡y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nğŸš€ Cháº¡y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TÄƒng cÆ°á»ng dá»¯ liá»‡u\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    X_aug = X_aug.reshape(X_aug.shape[0], X_aug.shape[1], 1)\n",
    "    \n",
    "    # Chia táº­p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Cháº¡y PSO\n",
    "    print(\"ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_cnn(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {best_params}\")\n",
    "    print(f\"ğŸ“‰ Score tá»‘t nháº¥t: {best_score:.4f}\")\n",
    "    \n",
    "    # Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In káº¿t quáº£\n",
    "    print(\"\\nğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"ğŸ“Œ {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"ğŸ“Œ {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # LÆ°u káº¿t quáº£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_results.csv', index=False)\n",
    "        print(f\"\\nÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/{dataset_name}_results.csv'\")\n",
    "    \n",
    "    # LÆ°u mÃ´ hÃ¬nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_model.keras')\n",
    "        print(f\"ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/{dataset_name}_model.keras'\")\n",
    "    \n",
    "    # Trá»±c quan hÃ³a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/{dataset_name}_results.png'\")\n",
    "\n",
    "# Cáº¥u hÃ¬nh cÃ¡c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bab446",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17844f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Cháº¡y pipeline cho dataset: desharnais\n",
      "ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\n",
      "\n",
      "ğŸ” Iteration 1/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1859\n",
      "\n",
      "ğŸ” Iteration 2/10\n",
      "\n",
      "ğŸ” Iteration 3/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1592\n",
      "\n",
      "ğŸ” Iteration 4/10\n",
      "\n",
      "ğŸ” Iteration 5/10\n",
      "\n",
      "ğŸ” Iteration 6/10\n",
      "\n",
      "ğŸ” Iteration 7/10\n",
      "\n",
      "ğŸ” Iteration 8/10\n",
      "\n",
      "ğŸ” Iteration 9/10\n",
      "\n",
      "ğŸ” Iteration 10/10\n",
      "ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {'hidden_units1': 35, 'hidden_units2': 11, 'l2_reg': np.float64(0.045730786735993637), 'dropout_rate': np.float64(0.243068412372354), 'learning_rate': np.float64(0.0044283623738321475), 'batch_size': 18, 'epochs': 71}\n",
      "ğŸ“‰ Score tá»‘t nháº¥t: 0.1592\n",
      "\n",
      "ğŸ“‚ Fold 1/3\n",
      "âœ… Fold 1 RMSE: 0.3559\n",
      "\n",
      "ğŸ“‚ Fold 2/3\n",
      "âœ… Fold 2 RMSE: 0.0884\n",
      "\n",
      "ğŸ“‚ Fold 3/3\n",
      "âœ… Fold 3 RMSE: 0.0669\n",
      "\n",
      "ğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\n",
      "ğŸ“Œ MAE    : 0.0634\n",
      "ğŸ“Œ MSE    : 0.0058\n",
      "ğŸ“Œ RMSE   : 0.0760\n",
      "ğŸ“Œ R2     : 0.9959\n",
      "ğŸ“Œ MMRE   : 0.5409\n",
      "ğŸ“Œ MDMRE  : 0.0520\n",
      "ğŸ“Œ PRED25 : 88.2353\n",
      "\n",
      "ÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/desharnais_mlp_results.csv'\n",
      "ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/desharnais_mlp_model.keras'\n",
      "ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/desharnais_mlp_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thiáº¿t láº­p seed Ä‘á»ƒ tÃ¡i láº­p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cáº¥u hÃ¬nh báº­t/táº¯t cÃ¡c bÆ°á»›c\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Báº­t/táº¯t tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "    'normalize_features': False,   # Báº­t/táº¯t chuáº©n hÃ³a Ä‘áº·c trÆ°ng\n",
    "    'log_transform_target': False, # Báº­t/táº¯t biáº¿n Ä‘á»•i log cho biáº¿n má»¥c tiÃªu\n",
    "    'save_model': True,            # Báº­t/táº¯t lÆ°u mÃ´ hÃ¬nh\n",
    "    'save_results': True,          # Báº­t/táº¯t lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "    'visualize_results': True,     # Báº­t/táº¯t trá»±c quan hÃ³a\n",
    "    'noise_factor': 0.01,          # Äá»™ lá»›n nhiá»…u Gaussian\n",
    "    'num_noise_copies': 2,         # Sá»‘ báº£n sao nhiá»…u\n",
    "    'test_size': 0.15,             # Tá»· lá»‡ táº­p test\n",
    "    'k_folds': 3,                  # Sá»‘ fold trong cross-validation\n",
    "}\n",
    "\n",
    "# HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# HÃ m Ä‘á»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiá»ƒm tra cá»™t tá»“n táº¡i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiá»ƒm tra kiá»ƒu dá»¯ liá»‡u\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiá»ƒm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuáº©n hÃ³a Ä‘áº·c trÆ°ng náº¿u báº­t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biáº¿n Ä‘á»•i log cho má»¥c tiÃªu náº¿u báº­t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# HÃ m tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# HÃ m xÃ¢y dá»±ng mÃ´ hÃ¬nh MLP\n",
    "def build_mlp_model(input_dim, hidden_units1=16, hidden_units2=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        Dense(hidden_units1, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(hidden_units2, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# KhÃ´ng gian siÃªu tham sá»‘ cho MLP\n",
    "param_bounds = {\n",
    "    'hidden_units1': (8, 32),\n",
    "    'hidden_units2': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a & giáº£i mÃ£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['hidden_units1'][0], param_bounds['hidden_units1'][1] + 1),\n",
    "        np.random.randint(param_bounds['hidden_units2'][0], param_bounds['hidden_units2'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'hidden_units1': int(particle[0]),\n",
    "        'hidden_units2': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# HÃ m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_mlp_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# HÃ m cháº¡y PSO\n",
    "def run_pso_mlp(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nğŸ” Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"âœ… Cáº­p nháº­t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# HÃ m huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh tá»‘i Æ°u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_mlp_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nğŸ“‚ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"âœ… Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # TÃ­nh trung bÃ¬nh lá»‹ch sá»­ huáº¥n luyá»‡n\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# HÃ m trá»±c quan hÃ³a káº¿t quáº£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bÃ¬nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_mlp_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# HÃ m chÃ­nh Ä‘á»ƒ cháº¡y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nğŸš€ Cháº¡y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TÄƒng cÆ°á»ng dá»¯ liá»‡u\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Chia táº­p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Cháº¡y PSO\n",
    "    print(\"ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_mlp(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {best_params}\")\n",
    "    print(f\"ğŸ“‰ Score tá»‘t nháº¥t: {best_score:.4f}\")\n",
    "    \n",
    "    # Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In káº¿t quáº£\n",
    "    print(\"\\nğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"ğŸ“Œ {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"ğŸ“Œ {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # LÆ°u káº¿t quáº£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_mlp_results.csv', index=False)\n",
    "        print(f\"\\nÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/{dataset_name}_mlp_results.csv'\")\n",
    "    \n",
    "    # LÆ°u mÃ´ hÃ¬nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_mlp_model.keras')\n",
    "        print(f\"ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/{dataset_name}_mlp_model.keras'\")\n",
    "    \n",
    "    # Trá»±c quan hÃ³a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/{dataset_name}_mlp_results.png'\")\n",
    "\n",
    "# Cáº¥u hÃ¬nh cÃ¡c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Cháº¡y pipeline cho tá»«ng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851a234",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Cháº¡y pipeline cho dataset: desharnais\n",
      "ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\n",
      "\n",
      "ğŸ” Iteration 1/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2331\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2070\n",
      "\n",
      "ğŸ” Iteration 2/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.2040\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1954\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1946\n",
      "\n",
      "ğŸ” Iteration 3/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1709\n",
      "\n",
      "ğŸ” Iteration 4/10\n",
      "\n",
      "ğŸ” Iteration 5/10\n",
      "\n",
      "ğŸ” Iteration 6/10\n",
      "\n",
      "ğŸ” Iteration 7/10\n",
      "\n",
      "ğŸ” Iteration 8/10\n",
      "\n",
      "ğŸ” Iteration 9/10\n",
      "\n",
      "ğŸ” Iteration 10/10\n",
      "ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {'lstm_units': 17, 'dense_units': 10, 'l2_reg': np.float64(0.05290511907499294), 'dropout_rate': np.float64(0.2388252925394604), 'learning_rate': np.float64(0.01028603584658312), 'batch_size': 13, 'epochs': 105}\n",
      "ğŸ“‰ Score tá»‘t nháº¥t: 0.1709\n",
      "\n",
      "ğŸ“‚ Fold 1/3\n",
      "âœ… Fold 1 RMSE: 0.1741\n",
      "\n",
      "ğŸ“‚ Fold 2/3\n",
      "âœ… Fold 2 RMSE: 0.1602\n",
      "\n",
      "ğŸ“‚ Fold 3/3\n",
      "âœ… Fold 3 RMSE: 0.1102\n",
      "\n",
      "ğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\n",
      "ğŸ“Œ MAE    : 0.1038\n",
      "ğŸ“Œ MSE    : 0.0185\n",
      "ğŸ“Œ RMSE   : 0.1361\n",
      "ğŸ“Œ R2     : 0.9869\n",
      "ğŸ“Œ MMRE   : 3.4108\n",
      "ğŸ“Œ MDMRE  : 0.0612\n",
      "ğŸ“Œ PRED25 : 88.2353\n",
      "\n",
      "ÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/desharnais_lstm_results.csv'\n",
      "ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/desharnais_lstm_model.keras'\n",
      "ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/desharnais_lstm_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thiáº¿t láº­p seed Ä‘á»ƒ tÃ¡i láº­p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cáº¥u hÃ¬nh báº­t/táº¯t cÃ¡c bÆ°á»›c\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Báº­t/táº¯t tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "    'normalize_features': False,   # Báº­t/táº¯t chuáº©n hÃ³a Ä‘áº·c trÆ°ng\n",
    "    'log_transform_target': False, # Báº­t/táº¯t biáº¿n Ä‘á»•i log cho biáº¿n má»¥c tiÃªu\n",
    "    'save_model': True,            # Báº­t/táº¯t lÆ°u mÃ´ hÃ¬nh\n",
    "    'save_results': True,          # Báº­t/táº¯t lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "    'visualize_results': True,     # Báº­t/táº¯t trá»±c quan hÃ³a\n",
    "    'noise_factor': 0.01,          # Äá»™ lá»›n nhiá»…u Gaussian\n",
    "    'num_noise_copies': 2,         # Sá»‘ báº£n sao nhiá»…u\n",
    "    'test_size': 0.15,             # Tá»· lá»‡ táº­p test\n",
    "    'k_folds': 3,                  # Sá»‘ fold trong cross-validation\n",
    "    'timesteps': 1                 # Sá»‘ bÆ°á»›c thá»i gian cho LSTM\n",
    "}\n",
    "\n",
    "# HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# HÃ m Ä‘á»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiá»ƒm tra cá»™t tá»“n táº¡i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiá»ƒm tra kiá»ƒu dá»¯ liá»‡u\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiá»ƒm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuáº©n hÃ³a Ä‘áº·c trÆ°ng náº¿u báº­t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biáº¿n Ä‘á»•i log cho má»¥c tiÃªu náº¿u báº­t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# HÃ m tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# HÃ m reshape dá»¯ liá»‡u cho LSTM\n",
    "def reshape_for_lstm(X, timesteps=1):\n",
    "    # Reshape dá»¯ liá»‡u thÃ nh [samples, timesteps, features]\n",
    "    n_samples, n_features = X.shape\n",
    "    n_timesteps = timesteps\n",
    "    n_new_samples = n_samples // n_timesteps\n",
    "    X_reshaped = X[:n_new_samples * n_timesteps].reshape(n_new_samples, n_timesteps, n_features)\n",
    "    return X_reshaped\n",
    "\n",
    "# HÃ m xÃ¢y dá»±ng mÃ´ hÃ¬nh LSTM\n",
    "def build_lstm_model(input_shape, lstm_units=16, dense_units=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(lstm_units, activation='tanh', recurrent_activation='sigmoid', return_sequences=False,\n",
    "             kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# KhÃ´ng gian siÃªu tham sá»‘ cho LSTM\n",
    "param_bounds = {\n",
    "    'lstm_units': (8, 32),\n",
    "    'dense_units': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a & giáº£i mÃ£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['lstm_units'][0], param_bounds['lstm_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'lstm_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# HÃ m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_lstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# HÃ m cháº¡y PSO\n",
    "def run_pso_lstm(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nğŸ” Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"âœ… Cáº­p nháº­t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# HÃ m huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh tá»‘i Æ°u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_lstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nğŸ“‚ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"âœ… Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # TÃ­nh trung bÃ¬nh lá»‹ch sá»­ huáº¥n luyá»‡n\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# HÃ m trá»±c quan hÃ³a káº¿t quáº£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bÃ¬nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_lstm_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# HÃ m chÃ­nh Ä‘á»ƒ cháº¡y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nğŸš€ Cháº¡y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TÄƒng cÆ°á»ng dá»¯ liá»‡u\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Reshape dá»¯ liá»‡u cho LSTM\n",
    "    X_aug = reshape_for_lstm(X_aug, CONFIG['timesteps'])\n",
    "    y_aug = y_aug[:X_aug.shape[0]]\n",
    "    \n",
    "    # Chia táº­p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Cháº¡y PSO\n",
    "    print(\"ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_lstm(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {best_params}\")\n",
    "    print(f\"ğŸ“‰ Score tá»‘t nháº¥t: {best_score:.4f}\")\n",
    "    \n",
    "    # Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In káº¿t quáº£\n",
    "    print(\"\\nğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"ğŸ“Œ {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"ğŸ“Œ {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # LÆ°u káº¿t quáº£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_lstm_results.csv', index=False)\n",
    "        print(f\"\\nÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/{dataset_name}_lstm_results.csv'\")\n",
    "    \n",
    "    # LÆ°u mÃ´ hÃ¬nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_lstm_model.keras')\n",
    "        print(f\"ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/{dataset_name}_lstm_model.keras'\")\n",
    "    \n",
    "    # Trá»±c quan hÃ³a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/{dataset_name}_lstm_results.png'\")\n",
    "\n",
    "# Cáº¥u hÃ¬nh cÃ¡c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Cháº¡y pipeline cho tá»«ng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a647c",
   "metadata": {},
   "source": [
    "# RBFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Cháº¡y pipeline cho dataset: desharnais\n",
      "ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\n",
      "\n",
      "ğŸ” Iteration 1/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.4931\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.4696\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.4233\n",
      "\n",
      "ğŸ” Iteration 2/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3826\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3691\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3667\n",
      "\n",
      "ğŸ” Iteration 3/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3663\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3646\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3622\n",
      "\n",
      "ğŸ” Iteration 4/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3620\n",
      "\n",
      "ğŸ” Iteration 5/10\n",
      "\n",
      "ğŸ” Iteration 6/10\n",
      "\n",
      "ğŸ” Iteration 7/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.3596\n",
      "\n",
      "ğŸ” Iteration 8/10\n",
      "\n",
      "ğŸ” Iteration 9/10\n",
      "\n",
      "ğŸ” Iteration 10/10\n",
      "ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {'num_centers': 22, 'sigma': 2.14376277835498, 'learning_rate': 0.008146268376391983, 'batch_size': 11, 'epochs': 43}\n",
      "ğŸ“‰ Score tá»‘t nháº¥t: 0.3596\n",
      "\n",
      "ğŸ“‚ Fold 1/3\n",
      "âœ… Fold 1 RMSE: 0.3454\n",
      "\n",
      "ğŸ“‚ Fold 2/3\n",
      "âœ… Fold 2 RMSE: 0.3723\n",
      "\n",
      "ğŸ“‚ Fold 3/3\n",
      "âœ… Fold 3 RMSE: 0.3257\n",
      "\n",
      "ğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\n",
      "ğŸ“Œ MAE    : 0.3087\n",
      "ğŸ“Œ MSE    : 0.2034\n",
      "ğŸ“Œ RMSE   : 0.4510\n",
      "ğŸ“Œ R2     : 0.8565\n",
      "ğŸ“Œ MMRE   : 2.0215\n",
      "ğŸ“Œ MDMRE  : 0.3395\n",
      "ğŸ“Œ PRED25 : 41.1765\n",
      "\n",
      "ÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/desharnais_rbfn_results.csv'\n",
      "ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/desharnais_rbfn_model.keras'\n",
      "ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/desharnais_rbfn_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thiáº¿t láº­p seed Ä‘á»ƒ tÃ¡i láº­p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cáº¥u hÃ¬nh báº­t/táº¯t cÃ¡c bÆ°á»›c\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Báº­t/táº¯t tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "    'normalize_features': False,   # Báº­t/táº¯t chuáº©n hÃ³a Ä‘áº·c trÆ°ng\n",
    "    'log_transform_target': False, # Báº­t/táº¯t biáº¿n Ä‘á»•i log cho biáº¿n má»¥c tiÃªu\n",
    "    'save_model': True,            # Báº­t/táº¯t lÆ°u mÃ´ hÃ¬nh\n",
    "    'save_results': True,          # Báº­t/táº¯t lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "    'visualize_results': True,     # Báº­t/táº¯t trá»±c quan hÃ³a\n",
    "    'noise_factor': 0.01,          # Äá»™ lá»›n nhiá»…u Gaussian\n",
    "    'num_noise_copies': 2,         # Sá»‘ báº£n sao nhiá»…u\n",
    "    'test_size': 0.15,             # Tá»· lá»‡ táº­p test\n",
    "    'k_folds': 3,                  # Sá»‘ fold trong cross-validation\n",
    "}\n",
    "\n",
    "# HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# HÃ m Ä‘á»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiá»ƒm tra cá»™t tá»“n táº¡i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiá»ƒm tra kiá»ƒu dá»¯ liá»‡u\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiá»ƒm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuáº©n hÃ³a Ä‘áº·c trÆ°ng náº¿u báº­t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biáº¿n Ä‘á»•i log cho má»¥c tiÃªu náº¿u báº­t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# HÃ m tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# Táº§ng RBF tÃ¹y chá»‰nh\n",
    "class RBFLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, sigma, centers, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.sigma = float(sigma)\n",
    "        self.centers = tf.Variable(centers, trainable=False, dtype=tf.float32, name='centers')\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.rbf_weights = self.add_weight(\n",
    "            name='rbf_weights',\n",
    "            shape=(self.units, 1),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.rbf_biases = self.add_weight(\n",
    "            name='rbf_biases',\n",
    "            shape=(1,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        diff = tf.expand_dims(inputs, axis=1) - tf.expand_dims(self.centers, axis=0)\n",
    "        l2 = tf.reduce_sum(tf.square(diff), axis=-1)\n",
    "        output = tf.exp(-l2 / (2.0 * self.sigma ** 2))\n",
    "        return tf.matmul(output, self.rbf_weights) + self.rbf_biases\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(RBFLayer, self).get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'sigma': float(self.sigma),\n",
    "            'centers': self.centers.numpy().tolist()\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# HÃ m xÃ¢y dá»±ng mÃ´ hÃ¬nh RBFN\n",
    "def build_rbfn_model(input_dim, num_centers=10, sigma=1.0, learning_rate=0.001, X_train=None):\n",
    "    if X_train is None:\n",
    "        raise ValueError(\"X_train must be provided to initialize centers with KMeans\")\n",
    "    \n",
    "    # Khá»Ÿi táº¡o centers báº±ng KMeans\n",
    "    n_clusters = max(int(num_centers), 1)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(X_train)\n",
    "    centers = np.array(kmeans.cluster_centers_, dtype=np.float32)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),\n",
    "        RBFLayer(units=int(num_centers), sigma=sigma, centers=centers, name='rbf_layer'),\n",
    "        Dense(1, activation='linear', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# KhÃ´ng gian siÃªu tham sá»‘ cho RBFN\n",
    "param_bounds = {\n",
    "    'num_centers': (5, 20),\n",
    "    'sigma': (0.1, 2.0),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a & giáº£i mÃ£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['num_centers'][0], param_bounds['num_centers'][1] + 1),\n",
    "        np.random.uniform(param_bounds['sigma'][0], param_bounds['sigma'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'num_centers': int(particle[0]),\n",
    "        'sigma': float(particle[1]),\n",
    "        'learning_rate': float(particle[2]),\n",
    "        'batch_size': int(particle[3]),\n",
    "        'epochs': int(particle[4])\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# HÃ m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_rbfn_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        X_train=X_train,\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# HÃ m cháº¡y PSO\n",
    "def run_pso_rbfn(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nğŸ” Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][1] = max(particles[i][1], param_bounds['sigma'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"âœ… Cáº­p nháº­t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# HÃ m huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh tá»‘i Æ°u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_rbfn_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        X_train=X_train,\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nğŸ“‚ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"âœ… Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # TÃ­nh trung bÃ¬nh lá»‹ch sá»­ huáº¥n luyá»‡n\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# HÃ m trá»±c quan hÃ³a káº¿t quáº£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bÃ¬nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_rbfn_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# HÃ m chÃ­nh Ä‘á»ƒ cháº¡y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nğŸš€ Cháº¡y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TÄƒng cÆ°á»ng dá»¯ liá»‡u\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Chia táº­p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Cháº¡y PSO\n",
    "    print(\"ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_rbfn(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {best_params}\")\n",
    "    print(f\"ğŸ“‰ Score tá»‘t nháº¥t: {best_score:.4f}\")\n",
    "    \n",
    "    # Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In káº¿t quáº£\n",
    "    print(\"\\nğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"ğŸ“Œ {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"ğŸ“Œ {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # LÆ°u káº¿t quáº£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_rbfn_results.csv', index=False)\n",
    "        print(f\"\\nÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/{dataset_name}_rbfn_results.csv'\")\n",
    "    \n",
    "    # LÆ°u mÃ´ hÃ¬nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_rbfn_model.keras')\n",
    "        print(f\"ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/{dataset_name}_rbfn_model.keras'\")\n",
    "    \n",
    "    # Trá»±c quan hÃ³a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/{dataset_name}_rbfn_results.png'\")\n",
    "\n",
    "# Cáº¥u hÃ¬nh cÃ¡c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Cháº¡y pipeline cho tá»«ng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18597b1b",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbde1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Cháº¡y pipeline cho dataset: desharnais\n",
      "ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\n",
      "\n",
      "ğŸ” Iteration 1/10\n",
      "\n",
      "ğŸ” Iteration 2/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1618\n",
      "\n",
      "ğŸ” Iteration 3/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1569\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1564\n",
      "\n",
      "ğŸ” Iteration 4/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1502\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1476\n",
      "\n",
      "ğŸ” Iteration 5/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1470\n",
      "\n",
      "ğŸ” Iteration 6/10\n",
      "\n",
      "ğŸ” Iteration 7/10\n",
      "\n",
      "ğŸ” Iteration 8/10\n",
      "\n",
      "ğŸ” Iteration 9/10\n",
      "\n",
      "ğŸ” Iteration 10/10\n",
      "ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {'rnn_units': 19, 'dense_units': 6, 'l2_reg': np.float64(0.04578883850996157), 'dropout_rate': np.float64(0.1755520424929638), 'learning_rate': np.float64(0.009449979979976523), 'batch_size': 13, 'epochs': 104}\n",
      "ğŸ“‰ Score tá»‘t nháº¥t: 0.1470\n",
      "\n",
      "ğŸ“Œ Fold 1/3\n",
      "âœ… Fold 1 RMSE: 0.1408\n",
      "\n",
      "ğŸ“Œ Fold 2/3\n",
      "âœ… Fold 2 RMSE: 0.1700\n",
      "\n",
      "ğŸ“Œ Fold 3/3\n",
      "âœ… Fold 3 RMSE: 0.1077\n",
      "\n",
      "ğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\n",
      "ğŸ“Œ MAE    : 0.0879\n",
      "ğŸ“Œ MSE    : 0.0145\n",
      "ğŸ“Œ RMSE   : 0.1205\n",
      "ğŸ“Œ R2     : 0.9897\n",
      "ğŸ“Œ MMRE   : 3.2531\n",
      "ğŸ“Œ MDMRE  : 0.0423\n",
      "ğŸ“Œ PRED25 : 88.2353\n",
      "\n",
      "ÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/desharnais_rnn_results.csv'\n",
      "ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/desharnais_rnn_model.keras'\n",
      "ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/desharnais_rnn_results.png'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thiáº¿t láº­p seed Ä‘á»ƒ tÃ¡i láº­p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cáº¥u hÃ¬nh báº­t/táº¯t cÃ¡c bÆ°á»›c\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Báº­t/táº¯t tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "    'normalize_features': False,   # Báº­t/táº¯t chuáº©n hÃ³a Ä‘áº·c trÆ°ng\n",
    "    'log_transform_target': False, # Báº­t/táº¯t biáº¿n Ä‘á»•i log cho biáº¿n má»¥c tiÃªu\n",
    "    'save_model': True,            # Báº­t/táº¯t lÆ°u mÃ´ hÃ¬nh\n",
    "    'save_results': True,          # Báº­t/táº¯t lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "    'visualize_results': True,     # Báº­t/táº¯t trá»±c quan hÃ³a\n",
    "    'noise_factor': 0.01,          # Äá»™ lá»›n nhiá»…u Gaussian\n",
    "    'num_noise_copies': 2,         # Sá»‘ báº£n sao nhiá»…u\n",
    "    'test_size': 0.15,             # Tá»· lá»‡ táº­p test\n",
    "    'k_folds': 3,                  # Sá»‘ fold trong cross-validation\n",
    "    'timesteps': 1                 # Sá»‘ bÆ°á»›c thá»i gian cho RNN \n",
    "}\n",
    "\n",
    "# HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# HÃ m Ä‘á»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiá»ƒm tra cá»™t tá»“n táº¡i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiá»ƒm tra kiá»ƒu dá»¯ liá»‡u\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiá»ƒm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuáº©n hÃ³a Ä‘áº·c trÆ°ng náº¿u báº­t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biáº¿n Ä‘á»•i log cho má»¥c tiÃªu náº¿u báº­t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# HÃ m tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# HÃ m reshape dá»¯ liá»‡u cho RNN\n",
    "def reshape_for_rnn(X, timesteps=1):\n",
    "    # Reshape dá»¯ liá»‡u thÃ nh [samples, timesteps, features]\n",
    "    n_samples, n_features = X.shape\n",
    "    n_timesteps = timesteps\n",
    "    n_new_samples = n_samples // n_timesteps\n",
    "    X_reshaped = X[:n_new_samples * n_timesteps].reshape(n_new_samples, n_timesteps, n_features)\n",
    "    return X_reshaped\n",
    "\n",
    "# HÃ m xÃ¢y dá»±ng mÃ´ hÃ¬nh RNN\n",
    "def build_rnn_model(input_shape, rnn_units=16, dense_units=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        SimpleRNN(rnn_units, activation='tanh', return_sequences=False, kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# KhÃ´ng gian siÃªu tham sá»‘ cho RNN\n",
    "param_bounds = {\n",
    "    'rnn_units': (8, 32),\n",
    "    'dense_units': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (16, 32),\n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a & giáº£i mÃ£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['rnn_units'][0], param_bounds['rnn_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'rnn_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# HÃ m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_rnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# HÃ m cháº¡y PSO\n",
    "def run_pso_rnn(X_train, y_train, num_particles=15, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nğŸ” Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand() * np.ones(dim)\n",
    "            r2 = np.random.uniform(0, 1, dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, -1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"âœ… Cáº­p nháº­t g_best: Score = {g_best_score:.4f}\")\n",
    "                \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# HÃ m huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh tá»‘i Æ°u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_rnn_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nğŸ“Œ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"âœ… Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # TÃ­nh trung bÃ¬nh lá»‹ch sá»­ huáº¥n luyá»‡n\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# HÃ m trá»±c quan hÃ³a káº¿t quáº£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bÃ¬nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_rnn_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# HÃ m chÃ­nh Ä‘á»ƒ cháº¡y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nğŸš€ Cháº¡y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TÄƒng cÆ°á»ng dá»¯ liá»‡u\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Reshape dá»¯ liá»‡u cho RNN\n",
    "    X_aug = reshape_for_rnn(X_aug, CONFIG['timesteps'])\n",
    "    y_aug = y_aug[:X_aug.shape[0]]  # Cáº¯t y Ä‘á»ƒ khá»›p sá»‘ máº«u sau reshape\n",
    "    \n",
    "    # Chia táº­p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Cháº¡y PSO\n",
    "    print(\"ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_rnn(X_train, y_train, num_particles=15, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {best_params}\")\n",
    "    print(f\"ğŸ“‰ Score tá»‘t nháº¥t: {best_score:.4f}\")\n",
    "    \n",
    "    # Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In káº¿t quáº£\n",
    "    print(\"\\nğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"ğŸ“Œ {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"ğŸ“Œ {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # LÆ°u káº¿t quáº£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_rnn_results.csv', index=False)\n",
    "        print(f\"\\nÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/{dataset_name}_rnn_results.csv'\")\n",
    "    \n",
    "    # LÆ°u mÃ´ hÃ¬nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_rnn_model.keras')\n",
    "        print(f\"ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/{dataset_name}_rnn_model.keras'\")\n",
    "    \n",
    "    # Trá»±c quan hÃ³a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/{dataset_name}_rnn_results.png'\")\n",
    "\n",
    "# Cáº¥u hÃ¬nh cÃ¡c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Cháº¡y pipeline cho tá»«ng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b6f04",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3850322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Cháº¡y pipeline cho dataset: desharnais\n",
      "ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\n",
      "\n",
      "ğŸ” Iteration 1/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1341\n",
      "\n",
      "ğŸ” Iteration 2/10\n",
      "\n",
      "ğŸ” Iteration 3/10\n",
      "âœ… Cáº­p nháº­t g_best: Score = 0.1324\n",
      "\n",
      "ğŸ” Iteration 4/10\n",
      "\n",
      "ğŸ” Iteration 5/10\n",
      "\n",
      "ğŸ” Iteration 6/10\n",
      "\n",
      "ğŸ” Iteration 7/10\n",
      "\n",
      "ğŸ” Iteration 8/10\n",
      "\n",
      "ğŸ” Iteration 9/10\n",
      "\n",
      "ğŸ” Iteration 10/10\n",
      "ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {'lstm_units': 10, 'dense_units': 1, 'l2_reg': np.float64(0.018236164417873006), 'dropout_rate': np.float64(0.32471572716955666), 'learning_rate': np.float64(0.005911359310043052), 'batch_size': 13, 'epochs': 112}\n",
      "ğŸ“‰ Score tá»‘t nháº¥t: 0.1324\n",
      "\n",
      "ğŸ“‚ Fold 1/3\n",
      "âœ… Fold 1 RMSE: 0.3708\n",
      "\n",
      "ğŸ“‚ Fold 2/3\n",
      "âœ… Fold 2 RMSE: 0.3822\n",
      "\n",
      "ğŸ“‚ Fold 3/3\n",
      "âœ… Fold 3 RMSE: 0.2721\n",
      "\n",
      "ğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\n",
      "ğŸ“Œ MAE    : 0.3196\n",
      "ğŸ“Œ MSE    : 0.1606\n",
      "ğŸ“Œ RMSE   : 0.4008\n",
      "ğŸ“Œ R2     : 0.8867\n",
      "ğŸ“Œ MMRE   : 2.4827\n",
      "ğŸ“Œ MDMRE  : 0.3384\n",
      "ğŸ“Œ PRED25 : 29.4118\n",
      "\n",
      "ÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/desharnais_bilstm_results.csv'\n",
      "ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/desharnais_bilstm_model.keras'\n",
      "ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/desharnais_bilstm_results.png'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Thiáº¿t láº­p seed Ä‘á»ƒ tÃ¡i láº­p\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Cáº¥u hÃ¬nh báº­t/táº¯t cÃ¡c bÆ°á»›c\n",
    "CONFIG = {\n",
    "    'apply_gaussian_noise': True,  # Báº­t/táº¯t tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "    'normalize_features': False,   # Báº­t/táº¯t chuáº©n hÃ³a Ä‘áº·c trÆ°ng\n",
    "    'log_transform_target': False, # Báº­t/táº¯t biáº¿n Ä‘á»•i log cho biáº¿n má»¥c tiÃªu\n",
    "    'save_model': True,            # Báº­t/táº¯t lÆ°u mÃ´ hÃ¬nh\n",
    "    'save_results': True,          # Báº­t/táº¯t lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "    'visualize_results': True,     # Báº­t/táº¯t trá»±c quan hÃ³a\n",
    "    'noise_factor': 0.01,          # Äá»™ lá»›n nhiá»…u Gaussian\n",
    "    'num_noise_copies': 2,         # Sá»‘ báº£n sao nhiá»…u\n",
    "    'test_size': 0.15,             # Tá»· lá»‡ táº­p test\n",
    "    'k_folds': 3,                  # Sá»‘ fold trong cross-validation\n",
    "    'timesteps': 1                 # Sá»‘ bÆ°á»›c thá»i gian cho BiLSTM \n",
    "}\n",
    "\n",
    "# HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mask = y_true > 0\n",
    "    if np.sum(mask) == 0:\n",
    "        return {\n",
    "            'mae': np.nan, 'mse': np.nan, 'rmse': np.nan, 'r2': np.nan,\n",
    "            'mmre': np.nan, 'mdmre': np.nan, 'pred25': np.nan\n",
    "        }\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mre = np.abs(y_true[mask] - y_pred[mask]) / y_true[mask]\n",
    "    mmre = np.mean(mre)\n",
    "    mdmre = np.median(mre)\n",
    "    pred25 = np.mean(mre <= 0.25) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
    "        'mmre': mmre, 'mdmre': mdmre, 'pred25': pred25\n",
    "    }\n",
    "\n",
    "# HÃ m Ä‘á»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "def load_and_preprocess_data(dataset_path, feature_columns, target_column, numeric_columns):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Kiá»ƒm tra cá»™t tá»“n táº¡i\n",
    "    missing_cols = [col for col in feature_columns + [target_column] if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_cols}\")\n",
    "    \n",
    "    # Kiá»ƒm tra kiá»ƒu dá»¯ liá»‡u\n",
    "    for col in numeric_columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            raise ValueError(f\"Column {col} is not numeric\")\n",
    "    if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column {target_column} is not numeric\")\n",
    "    \n",
    "    # Kiá»ƒm tra NaN\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isnan(y)):\n",
    "        raise ValueError(\"Data contains NaN values\")\n",
    "    \n",
    "    # Chuáº©n hÃ³a Ä‘áº·c trÆ°ng náº¿u báº­t\n",
    "    if CONFIG['normalize_features']:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    \n",
    "    # Biáº¿n Ä‘á»•i log cho má»¥c tiÃªu náº¿u báº­t\n",
    "    if CONFIG['log_transform_target']:\n",
    "        y = np.log1p(y)\n",
    "    \n",
    "    return X, y, df\n",
    "\n",
    "# HÃ m tÄƒng cÆ°á»ng dá»¯ liá»‡u báº±ng nhiá»…u Gaussian\n",
    "def augment_data(X, y, noise_factor=0.01, num_copies=2):\n",
    "    if not CONFIG['apply_gaussian_noise']:\n",
    "        return X, y\n",
    "    \n",
    "    X_aug = X.copy()\n",
    "    y_aug = y.copy()\n",
    "    \n",
    "    for _ in range(num_copies):\n",
    "        noise = np.random.normal(loc=0, scale=noise_factor, size=X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_aug = np.vstack((X_aug, X_noisy))\n",
    "        y_aug = np.hstack((y_aug, y))\n",
    "    \n",
    "    return X_aug, y_aug\n",
    "\n",
    "# HÃ m reshape dá»¯ liá»‡u cho BiLSTM\n",
    "def reshape_for_bilstm(X, timesteps=1):\n",
    "    # Reshape dá»¯ liá»‡u thÃ nh [samples, timesteps, features]\n",
    "    n_samples, n_features = X.shape\n",
    "    n_timesteps = timesteps\n",
    "    n_new_samples = n_samples // n_timesteps\n",
    "    X_reshaped = X[:n_new_samples * n_timesteps].reshape(n_new_samples, n_timesteps, n_features)\n",
    "    return X_reshaped\n",
    "\n",
    "# HÃ m xÃ¢y dá»±ng mÃ´ hÃ¬nh BiLSTM\n",
    "def build_bilstm_model(input_shape, lstm_units=16, dense_units=8, l2_reg=0.01, dropout_rate=0.3, learning_rate=0.001):\n",
    "    l2_reg = max(l2_reg, 0.001)\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=False, kernel_regularizer=l2(l2_reg))),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# KhÃ´ng gian siÃªu tham sá»‘ cho BiLSTM\n",
    "param_bounds = {\n",
    "    'lstm_units': (8, 16),  \n",
    "    'dense_units': (4, 16),\n",
    "    'l2_reg': (0.001, 0.05),\n",
    "    'dropout_rate': (0.2, 0.4),\n",
    "    'learning_rate': (1e-4, 1e-2),\n",
    "    'batch_size': (4, 16), \n",
    "    'epochs': (50, 100)\n",
    "}\n",
    "\n",
    "# HÃ m mÃ£ hÃ³a & giáº£i mÃ£ particle\n",
    "def random_particle():\n",
    "    return np.array([\n",
    "        np.random.randint(param_bounds['lstm_units'][0], param_bounds['lstm_units'][1] + 1),\n",
    "        np.random.randint(param_bounds['dense_units'][0], param_bounds['dense_units'][1] + 1),\n",
    "        np.random.uniform(param_bounds['l2_reg'][0], param_bounds['l2_reg'][1]),\n",
    "        np.random.uniform(param_bounds['dropout_rate'][0], param_bounds['dropout_rate'][1]),\n",
    "        np.random.uniform(param_bounds['learning_rate'][0], param_bounds['learning_rate'][1]),\n",
    "        np.random.randint(param_bounds['batch_size'][0], param_bounds['batch_size'][1] + 1),\n",
    "        np.random.randint(param_bounds['epochs'][0], param_bounds['epochs'][1] + 1)\n",
    "    ])\n",
    "\n",
    "def decode_particle(particle):\n",
    "    params = {\n",
    "        'lstm_units': int(particle[0]),\n",
    "        'dense_units': int(particle[1]),\n",
    "        'l2_reg': particle[2],\n",
    "        'dropout_rate': particle[3],\n",
    "        'learning_rate': particle[4],\n",
    "        'batch_size': int(particle[5]),\n",
    "        'epochs': int(particle[6])\n",
    "    }\n",
    "    params['l2_reg'] = max(params['l2_reg'], 0.001)\n",
    "    return params\n",
    "\n",
    "# HÃ m fitness cho PSO\n",
    "def fitness_function(particle, X_train, y_train):\n",
    "    params = decode_particle(particle)\n",
    "    model = build_bilstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        \n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'],\n",
    "                  validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr])\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "# HÃ m cháº¡y PSO\n",
    "def run_pso_bilstm(X_train, y_train, num_particles=10, max_iter=10):\n",
    "    dim = len(param_bounds)\n",
    "    bounds_array = np.array(list(param_bounds.values()))\n",
    "    \n",
    "    particles = [random_particle() for _ in range(num_particles)]\n",
    "    velocities = [np.zeros(dim) for _ in range(num_particles)]\n",
    "    p_best_positions = particles.copy()\n",
    "    p_best_scores = [fitness_function(p, X_train, y_train) for p in particles]\n",
    "    \n",
    "    g_best_index = np.argmin(p_best_scores)\n",
    "    g_best_position = p_best_positions[g_best_index]\n",
    "    g_best_score = p_best_scores[g_best_index]\n",
    "    \n",
    "    w, c1, c2 = 0.5, 1.5, 1.5\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        print(f\"\\nğŸ” Iteration {iter + 1}/{max_iter}\")\n",
    "        for i in range(num_particles):\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "            \n",
    "            velocities[i] = (\n",
    "                w * velocities[i]\n",
    "                + c1 * r1 * (p_best_positions[i] - particles[i])\n",
    "                + c2 * r2 * (g_best_position - particles[i])\n",
    "            )\n",
    "            \n",
    "            particles[i] += velocities[i]\n",
    "            particles[i] = np.clip(particles[i], bounds_array[:, 0], bounds_array[:, 1])\n",
    "            particles[i][2] = max(particles[i][2], param_bounds['l2_reg'][0])\n",
    "            \n",
    "            score = fitness_function(particles[i], X_train, y_train)\n",
    "            \n",
    "            if score < p_best_scores[i]:\n",
    "                p_best_scores[i] = score\n",
    "                p_best_positions[i] = particles[i]\n",
    "                \n",
    "            if score < g_best_score:\n",
    "                g_best_score = score\n",
    "                g_best_position = particles[i]\n",
    "                print(f\"âœ… Cáº­p nháº­t g_best: Score = {g_best_score:.4f}\")\n",
    "    \n",
    "    return g_best_position, g_best_score\n",
    "\n",
    "# HÃ m huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh tá»‘i Æ°u\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_bilstm_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        **{k: v for k, v in best_params.items() if k not in ['batch_size', 'epochs']}\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    kf = KFold(n_splits=CONFIG['k_folds'], shuffle=True, random_state=42)\n",
    "    metrics_all = []\n",
    "    history_all = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"\\nğŸ“‚ Fold {fold + 1}/{CONFIG['k_folds']}\")\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_tr, y_tr, epochs=best_params['epochs'], batch_size=best_params['batch_size'],\n",
    "            validation_split=0.2, verbose=0, callbacks=[early_stopping, reduce_lr]\n",
    "        )\n",
    "        y_pred = model.predict(X_val, verbose=0).flatten()\n",
    "        metrics = calculate_metrics(y_val, y_pred)\n",
    "        metrics_all.append(metrics)\n",
    "        print(f\"âœ… Fold {fold + 1} RMSE: {metrics['rmse']:.4f}\")\n",
    "        \n",
    "        history_all['loss'].append(history.history['loss'])\n",
    "        history_all['val_loss'].append(history.history['val_loss'])\n",
    "    \n",
    "    # ÄÃ¡nh giÃ¡ trÃªn táº­p test\n",
    "    y_pred_test = model.predict(X_test, verbose=0).flatten()\n",
    "    test_metrics = calculate_metrics(y_test, y_pred_test)\n",
    "    \n",
    "    # TÃ­nh trung bÃ¬nh lá»‹ch sá»­ huáº¥n luyá»‡n\n",
    "    max_len = max(len(h) for h in history_all['loss'])\n",
    "    loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['loss']], axis=0)\n",
    "    val_loss_avg = np.mean([np.pad(h, (0, max_len - len(h)), 'constant', constant_values=np.nan) for h in history_all['val_loss']], axis=0)\n",
    "    \n",
    "    return model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test\n",
    "\n",
    "# HÃ m trá»±c quan hÃ³a káº¿t quáº£\n",
    "def visualize_results(dataset_name, y_test, y_pred, loss_avg, val_loss_avg):\n",
    "    if not CONFIG['visualize_results']:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Loss trung bÃ¬nh\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(loss_avg, label='Training Loss')\n",
    "    plt.plot(val_loss_avg, label='Validation Loss')\n",
    "    plt.title('Average Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Predicted vs Actual Effort')\n",
    "    plt.xlabel('Actual Effort')\n",
    "    plt.ylabel('Predicted Effort')\n",
    "    \n",
    "    # Error Distribution\n",
    "    errors = y_test - y_pred\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    plt.savefig(f'visualizations/{dataset_name}_bilstm_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# HÃ m chÃ­nh Ä‘á»ƒ cháº¡y pipeline\n",
    "def run_pipeline(dataset_config):\n",
    "    dataset_name = dataset_config['name']\n",
    "    print(f\"\\nğŸš€ Cháº¡y pipeline cho dataset: {dataset_name}\")\n",
    "    \n",
    "    # Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u\n",
    "    X, y, df = load_and_preprocess_data(\n",
    "        dataset_config['path'],\n",
    "        dataset_config['feature_columns'],\n",
    "        dataset_config['target_column'],\n",
    "        dataset_config['numeric_columns']\n",
    "    )\n",
    "    \n",
    "    # TÄƒng cÆ°á»ng dá»¯ liá»‡u\n",
    "    X_aug, y_aug = augment_data(X, y, CONFIG['noise_factor'], CONFIG['num_noise_copies'])\n",
    "    \n",
    "    # Reshape dá»¯ liá»‡u cho BiLSTM\n",
    "    X_aug = reshape_for_bilstm(X_aug, CONFIG['timesteps'])\n",
    "    y_aug = y_aug[:X_aug.shape[0]]  # Cáº¯t y Ä‘á»ƒ khá»›p sá»‘ máº«u sau reshape\n",
    "    \n",
    "    # Chia táº­p train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_aug, y_aug, test_size=CONFIG['test_size'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Cháº¡y PSO\n",
    "    print(\"ğŸ” TÃ¬m siÃªu tham sá»‘ tá»‘i Æ°u báº±ng PSO...\")\n",
    "    best_particle, best_score = run_pso_bilstm(X_train, y_train, num_particles=10, max_iter=10)\n",
    "    best_params = decode_particle(best_particle)\n",
    "    print(f\"ğŸ† SiÃªu tham sá»‘ tá»‘t nháº¥t: {best_params}\")\n",
    "    print(f\"ğŸ“‰ Score tá»‘t nháº¥t: {best_score:.4f}\")\n",
    "    \n",
    "    # Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡\n",
    "    model, test_metrics, metrics_all, loss_avg, val_loss_avg, y_pred_test = train_and_evaluate(\n",
    "        X_train, X_test, y_train, y_test, best_params\n",
    "    )\n",
    "    \n",
    "    # In káº¿t quáº£\n",
    "    print(\"\\nğŸ“ˆ Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn táº­p test:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"ğŸ“Œ {metric.upper():<7}: {value:.4f}\" if not np.isnan(value) else f\"ğŸ“Œ {metric.upper():<7}: NaN\")\n",
    "    \n",
    "    # LÆ°u káº¿t quáº£\n",
    "    if CONFIG['save_results']:\n",
    "        results_df = pd.DataFrame([test_metrics])\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        results_df.to_csv(f'results/{dataset_name}_bilstm_results.csv', index=False)\n",
    "        print(f\"\\nÄÃ£ lÆ°u káº¿t quáº£ vÃ o 'results/{dataset_name}_bilstm_results.csv'\")\n",
    "    \n",
    "    # LÆ°u mÃ´ hÃ¬nh\n",
    "    if CONFIG['save_model']:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        model.save(f'models/{dataset_name}_bilstm_model.keras')\n",
    "        print(f\"ÄÃ£ lÆ°u mÃ´ hÃ¬nh vÃ o 'models/{dataset_name}_bilstm_model.keras'\")\n",
    "    \n",
    "    # Trá»±c quan hÃ³a\n",
    "    visualize_results(dataset_name, y_test, y_pred_test, loss_avg, val_loss_avg)\n",
    "    print(f\"ÄÃ£ lÆ°u hÃ¬nh áº£nh trá»±c quan hÃ³a vÃ o 'visualizations/{dataset_name}_bilstm_results.png'\")\n",
    "\n",
    "# Cáº¥u hÃ¬nh cÃ¡c dataset\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'desharnais',\n",
    "        'path': 'desharnais1.1_processed_corrected.csv',\n",
    "        'feature_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ],\n",
    "        'target_column': 'Effort',\n",
    "        'numeric_columns': [\n",
    "            'TeamExp', 'ManagerExp', 'YearEnd', 'Length', 'Transactions', 'Entities',\n",
    "            'Adjustment', 'PointsAjust', 'StartYear', 'ProjectDurationYears',\n",
    "            'Transactions_Entities', 'Effort_PointsAjust', 'Effort_per_PointsAjust',\n",
    "            'Transactions_per_Entities', 'Language_b\\'1\\'', 'Language_b\\'2\\'',\n",
    "            'Language_b\\'3\\'', 'HighEffort', 'HighPointsAjust'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Cháº¡y pipeline cho tá»«ng dataset\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset in datasets:\n",
    "        run_pipeline(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
